{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16593246-71b6-417e-85c2-cbb72cfeca8e",
   "metadata": {},
   "source": [
    "### nn boop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074d1ba0-ffc7-4a73-8349-cb76bf84459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from torch.multiprocessing import set_start_method\n",
    "\n",
    "# Initialize multiprocessing method\n",
    "try:\n",
    "    set_start_method('spawn')\n",
    "except RuntimeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "891115a3-7c2d-41fc-a937-bb21869da5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Neural_Recommender\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4e10e",
   "metadata": {},
   "source": [
    "* Load as PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fffb529b-0351-45d0-a66b-7b150ac2e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, ratings_data):\n",
    "        self.users = torch.LongTensor(ratings_data['user_idx'].values)\n",
    "        self.movies = torch.LongTensor(ratings_data['movie_idx'].values)\n",
    "        self.ratings = torch.FloatTensor(ratings_data['rating'].values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'user_idx': self.users[idx],\n",
    "            'movie_idx': self.movies[idx],\n",
    "            'rating': self.ratings[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c0e92",
   "metadata": {},
   "source": [
    "* NCF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e95e0b8-e837-4c10-9fb7-4ab6d1264aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(pl.LightningModule):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64, layers=[256, 128, 64], lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # GMF part\n",
    "        self.user_embedding_gmf = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding_gmf = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # MLP part\n",
    "        self.user_embedding_mlp = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding_mlp = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # MLP layers\n",
    "        mlp_layers = []\n",
    "        input_dim = embedding_dim * 2\n",
    "        for layer_size in layers:\n",
    "            mlp_layers.extend([\n",
    "                nn.Linear(input_dim, layer_size),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(layer_size),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            input_dim = layer_size\n",
    "        \n",
    "        self.mlp_layers = nn.Sequential(*mlp_layers)\n",
    "        self.final_layer = nn.Linear(layers[-1] + embedding_dim, 1)\n",
    "        self.lr = lr\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        # GMF part\n",
    "        user_embed_gmf = self.user_embedding_gmf(user_indices)\n",
    "        item_embed_gmf = self.item_embedding_gmf(item_indices)\n",
    "        gmf_output = user_embed_gmf * item_embed_gmf\n",
    "        \n",
    "        # MLP part\n",
    "        user_embed_mlp = self.user_embedding_mlp(user_indices)\n",
    "        item_embed_mlp = self.item_embedding_mlp(item_indices)\n",
    "        mlp_input = torch.cat([user_embed_mlp, item_embed_mlp], dim=-1)\n",
    "        mlp_output = self.mlp_layers(mlp_input)\n",
    "        \n",
    "        combined = torch.cat([gmf_output, mlp_output], dim=-1)\n",
    "        return self.final_layer(combined).squeeze()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_indices = batch['user_idx']\n",
    "        item_indices = batch['movie_idx']\n",
    "        ratings = batch['rating']\n",
    "        \n",
    "        predictions = self(user_indices, item_indices)\n",
    "        loss = nn.MSELoss()(predictions, ratings)\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        user_indices = batch['user_idx']\n",
    "        item_indices = batch['movie_idx']\n",
    "        ratings = batch['rating']\n",
    "        \n",
    "        predictions = self(user_indices, item_indices)\n",
    "        loss = nn.MSELoss()(predictions, ratings)\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        user_indices = batch['user_idx']\n",
    "        item_indices = batch['movie_idx']\n",
    "        ratings = batch['rating']\n",
    "        \n",
    "        predictions = self(user_indices, item_indices)\n",
    "        loss = nn.MSELoss()(predictions, ratings)\n",
    "        \n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.1, patience=5, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94d890c0-8989-4c8c-9497-f4c1470d6391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sample_size):\n",
    "    ratings_df = spark.read.csv(\"ml-32m/ratings.csv\", header=True) \\\n",
    "        .withColumn(\"rating\", col(\"rating\").cast(\"double\")) \\\n",
    "        .withColumn(\"userId\", col(\"userId\").cast(\"integer\")) \\\n",
    "        .withColumn(\"movieId\", col(\"movieId\").cast(\"integer\"))\n",
    "    \n",
    "    ratings_df = ratings_df.sample(False, fraction=sample_size/ratings_df.count(), seed=42)\n",
    "    \n",
    "    ratings_pd = ratings_df.toPandas()\n",
    "    movies_df = spark.read.csv(\"ml-32m/movies.csv\", header=True).toPandas()\n",
    "    \n",
    "    print(f\"Total ratings loaded: {len(ratings_pd)}\")\n",
    "    \n",
    "    user_encoder = LabelEncoder()\n",
    "    movie_encoder = LabelEncoder()\n",
    "    \n",
    "    ratings_pd['user_idx'] = user_encoder.fit_transform(ratings_pd['userId'])\n",
    "    ratings_pd['movie_idx'] = movie_encoder.fit_transform(ratings_pd['movieId'])\n",
    "    \n",
    "    train_data, test_data = train_test_split(ratings_pd, test_size=0.2, random_state=42)\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return train_data, val_data, test_data, user_encoder, movie_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f10f7-1f2e-4a9c-a8ff-18fda1d7665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading and preparing data...\")\n",
    "train_data, val_data, test_data, user_encoder, movie_encoder = prepare_data(sample_size=30000000)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    MovieLensDataset(train_data),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    MovieLensDataset(val_data),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    MovieLensDataset(test_data),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "725a3fbc-1f43-433c-957c-be1f8450ef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = NCF(\n",
    "    num_users=len(user_encoder.classes_),\n",
    "    num_items=len(movie_encoder.classes_),\n",
    "    embedding_dim=16,\n",
    "    layers=[32, 24, 16],\n",
    "    lr=1e-3\n",
    ")\n",
    "# Setup trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(monitor='val_loss', mode='min'),\n",
    "        TQDMProgressBar(refresh_rate=1)\n",
    "    ],\n",
    "    enable_checkpointing=True,\n",
    "    logger=False,\n",
    "    num_sanity_val_steps=0\n",
    ")\n",
    "\n",
    "model = model.cuda()\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "print(next(model.parameters()).device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ab9a1a6-2cd8-4e07-acf7-3fa514d14d90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Users\\User\\big_data_project\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name               | Type       | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | user_embedding_gmf | Embedding  | 3.2 M  | train\n",
      "1 | item_embedding_gmf | Embedding  | 798 K  | train\n",
      "2 | user_embedding_mlp | Embedding  | 3.2 M  | train\n",
      "3 | item_embedding_mlp | Embedding  | 798 K  | train\n",
      "4 | mlp_layers         | Sequential | 2.4 K  | train\n",
      "5 | final_layer        | Linear     | 33     | train\n",
      "----------------------------------------------------------\n",
      "8.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.0 M     Total params\n",
      "32.024    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06cfde1cafe432c9549078e5ee31902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"Training model...\")\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e4b30a0-b161-4376-af91-de6aee151655",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(pl.Callback):\n",
    "    def __init__(self, user_encoder, movie_encoder, k=1, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.user_encoder = user_encoder\n",
    "        self.movie_encoder = movie_encoder\n",
    "        self.k = k\n",
    "        self.batch_size = batch_size\n",
    "        self.reset_metrics()\n",
    "        \n",
    "    def reset_metrics(self):\n",
    "        self.total_rmse = 0.0\n",
    "        self.total_mae = 0.0\n",
    "        self.hit_ratio = 0.0\n",
    "        self.ndcg = 0.0\n",
    "        self.predictions = []\n",
    "        self.actuals = []\n",
    "        self.n_users = 0\n",
    "        self.n_batches = 0\n",
    "    \n",
    "    def on_test_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        device = next(pl_module.parameters()).device\n",
    "        \n",
    "        if len(batch['user_idx']) == 0:\n",
    "            return\n",
    "            \n",
    "        user_indices = batch['user_idx'].to(device)\n",
    "        item_indices = batch['movie_idx'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "        \n",
    "        try:\n",
    "            pred = pl_module(user_indices, item_indices)\n",
    "            self.predictions.extend(pred.cpu().numpy())\n",
    "            self.actuals.extend(ratings.cpu().numpy())\n",
    "            \n",
    "            if len(pred) > 0:\n",
    "                self.total_rmse += torch.sqrt(nn.MSELoss()(pred, ratings)).item()\n",
    "                self.total_mae += torch.abs(pred - ratings).mean().item()\n",
    "                self.n_batches += 1\n",
    "            \n",
    "            unique_users = torch.unique(user_indices)\n",
    "            self.n_users += len(unique_users)\n",
    "            \n",
    "            # Batch recommendations\n",
    "            all_items = torch.arange(len(self.movie_encoder.classes_)).to(device)\n",
    "            \n",
    "            for i in range(0, len(unique_users), self.batch_size):\n",
    "                user_batch = unique_users[i:i + self.batch_size]\n",
    "                user_tensor = user_batch.repeat_interleave(len(all_items))\n",
    "                item_tensor = all_items.repeat(len(user_batch))\n",
    "                \n",
    "                all_predictions = pl_module(user_tensor, item_tensor).reshape(len(user_batch), -1)\n",
    "                _, indices = torch.topk(all_predictions, k=self.k, dim=1)\n",
    "                recommended_items = all_items[indices]\n",
    "                \n",
    "                for j, user_idx in enumerate(user_batch):\n",
    "                    user_mask = user_indices == user_idx\n",
    "                    actual_items = item_indices[user_mask]\n",
    "                    actual_ratings = ratings[user_mask]\n",
    "                    \n",
    "                    if len(actual_items) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    positive_items = actual_items[actual_ratings >= 4.0]\n",
    "                    if len(positive_items) > 0:\n",
    "                        hit = torch.any(torch.isin(recommended_items[j], positive_items)).item()\n",
    "                        self.hit_ratio += hit\n",
    "                    \n",
    "                    relevance = torch.zeros_like(recommended_items[j], dtype=torch.float, device=device)\n",
    "                    for k_idx, item in enumerate(recommended_items[j]):\n",
    "                        if item in actual_items:\n",
    "                            rating_idx = (actual_items == item).nonzero().item()\n",
    "                            if actual_ratings[rating_idx] >= 4.0:\n",
    "                                relevance[k_idx] = 1.0\n",
    "                    \n",
    "                    log2_range = torch.arange(2, len(relevance) + 2, dtype=torch.float, device=device)\n",
    "                    dcg = torch.sum(relevance / torch.log2(log2_range))\n",
    "                    ideal_relevance = torch.sort(relevance, descending=True)[0]\n",
    "                    idcg = torch.sum(ideal_relevance / torch.log2(log2_range))\n",
    "                    \n",
    "                    if idcg > 0:\n",
    "                        self.ndcg += (dcg / idcg).item()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "    \n",
    "    def on_test_epoch_end(self, trainer, pl_module):\n",
    "        metrics = {}\n",
    "        metrics['RMSE'] = self.total_rmse / self.n_batches if self.n_batches > 0 else float('nan')\n",
    "        metrics['MAE'] = self.total_mae / self.n_batches if self.n_batches > 0 else float('nan')\n",
    "        metrics['HR@50'] = self.hit_ratio / self.n_users if self.n_users > 0 else float('nan')\n",
    "        metrics['NDCG@50'] = self.ndcg / self.n_users if self.n_users > 0 else float('nan')\n",
    "        metrics['Coverage'] = len(set(self.predictions)) / len(self.movie_encoder.classes_) if len(self.movie_encoder.classes_) > 0 else float('nan')\n",
    "        \n",
    "        pl_module.log_dict(metrics)\n",
    "        self.print_metrics(metrics)\n",
    "        self.reset_metrics()\n",
    "        return metrics\n",
    "    \n",
    "    def print_metrics(self, metrics):\n",
    "        print(\"\\nTest Metrics:\")\n",
    "        print(f\"RMSE: {metrics['RMSE']:.4f}\")\n",
    "        print(f\"MAE: {metrics['MAE']:.4f}\")\n",
    "        print(f\"Hit Ratio@50: {metrics['HR@50']:.4f}\")\n",
    "        print(f\"NDCG@50: {metrics['NDCG@50']:.4f}\")\n",
    "        print(f\"Coverage: {metrics['Coverage']:.4f}\")\n",
    "\n",
    "def evaluate_model(model, test_loader, user_encoder, movie_encoder, k=1):\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "        callbacks=[MetricsCallback(user_encoder, movie_encoder, k=k)],\n",
    "        logger=False\n",
    "    )\n",
    "    trainer.test(model, test_loader)\n",
    "\n",
    "def get_top_k_recommendations(model, user_id, user_encoder, movie_encoder, k=1):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        user_idx = torch.tensor(user_encoder.transform([user_id])[0]).to(device)\n",
    "        all_items = torch.arange(len(movie_encoder.classes_)).to(device)\n",
    "        user_tensor = torch.full_like(all_items, user_idx)\n",
    "        \n",
    "        predictions = model(user_tensor, all_items)\n",
    "        _, indices = torch.topk(predictions, k=k)\n",
    "        recommended_items = all_items[indices].cpu().numpy()\n",
    "        predicted_ratings = predictions[indices].cpu().numpy()\n",
    "        \n",
    "        return list(zip(movie_encoder.inverse_transform(recommended_items), predicted_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2553f9ef-dc1f-4a20-97c6-78b18be17084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a516be9a2843b295adecfbe80a5c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Metrics:\n",
      "RMSE: 0.9158\n",
      "MAE: 0.7099\n",
      "Hit Ratio@50: 0.0028\n",
      "NDCG@50: 0.0028\n",
      "Coverage: 18.2860\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         Coverage          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     18.28596305847168     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           HR@50           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0027789755258709192   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">            MAE            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7098760008811951     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          NDCG@50          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0027789755258709192   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           RMSE            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9157717227935791     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8579136729240417     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        Coverage         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    18.28596305847168    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m          HR@50          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0027789755258709192  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m           MAE           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7098760008811951    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         NDCG@50         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0027789755258709192  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m          RMSE           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9157717227935791    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8579136729240417    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, user_encoder, movie_encoder, k=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1380f71-c608-4a36-bc93-27272a3c47b3",
   "metadata": {},
   "source": [
    "### Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0f13ab2-946b-4b7d-a481-aa784ff5ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(model, user_encoder, movie_encoder, save_dir=\"savedt_model\"):\n",
    "    \"\"\"\n",
    "    Save the model, encoders, and metadata to disk.\n",
    "    \n",
    "    Args:\n",
    "        model (NCF): The trained model\n",
    "        user_encoder (LabelEncoder): The fitted user encoder\n",
    "        movie_encoder (LabelEncoder): The fitted movie encoder\n",
    "        save_dir (str): Directory to save the model files\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict directly\n",
    "    torch.save(model.state_dict(), save_dir / \"model.pt\")\n",
    "    \n",
    "    # Save model hyperparameters\n",
    "    torch.save({\n",
    "        'num_users': len(user_encoder.classes_),\n",
    "        'num_items': len(movie_encoder.classes_),\n",
    "        'embedding_dim': model.hparams.embedding_dim,\n",
    "        'layers': model.hparams.layers,\n",
    "        'lr': model.hparams.lr\n",
    "    }, save_dir / \"model_config.pt\")\n",
    "    \n",
    "    # Save encoders\n",
    "    with open(save_dir / \"encoders.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            'user_encoder': user_encoder,\n",
    "            'movie_encoder': movie_encoder\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"Model and encoders saved to {save_dir}\")\n",
    "\n",
    "def load_model(load_dir=\"saved_model\"):\n",
    "    \"\"\"\n",
    "    Load the model, encoders, and metadata from disk.\n",
    "    \n",
    "    Args:\n",
    "        load_dir (str): Directory containing the saved model files\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (loaded_model, user_encoder, movie_encoder)\n",
    "    \"\"\"\n",
    "    load_dir = Path(load_dir)\n",
    "    \n",
    "    # Load model configuration\n",
    "    config = torch.load(load_dir / \"model_config.pt\")\n",
    "    \n",
    "    # Initialize model with saved configuration\n",
    "    model = NCF(\n",
    "        num_users=config['num_users'],\n",
    "        num_items=config['num_items'],\n",
    "        embedding_dim=config['embedding_dim'],\n",
    "        layers=config['layers'],\n",
    "        lr=config['lr']\n",
    "    )\n",
    "    \n",
    "    # Load model state dictionary\n",
    "    state_dict = torch.load(load_dir / \"model.pt\")\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    # Load encoders\n",
    "    with open(load_dir / \"encoders.pkl\", \"rb\") as f:\n",
    "        encoders = pickle.load(f)\n",
    "        user_encoder = encoders['user_encoder']\n",
    "        movie_encoder = encoders['movie_encoder']\n",
    "    \n",
    "    print(f\"Model and encoders loaded from {load_dir}\")\n",
    "    return model, user_encoder, movie_encoder\n",
    "\n",
    "def save_checkpoint(model, user_encoder, movie_encoder, epoch, metrics, optimizer, save_dir=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    Save a training checkpoint with model state and metrics.\n",
    "    \n",
    "    Args:\n",
    "        model (NCF): The model being trained\n",
    "        user_encoder (LabelEncoder): The user encoder\n",
    "        movie_encoder (LabelEncoder): The movie encoder\n",
    "        epoch (int): Current epoch number\n",
    "        metrics (dict): Dictionary of current metrics\n",
    "        optimizer (torch.optim.Optimizer): The optimizer\n",
    "        save_dir (str): Directory to save checkpoints\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict() if optimizer is not None else None,\n",
    "        'metrics': metrics,\n",
    "        'model_hparams': {\n",
    "            'num_users': len(user_encoder.classes_),\n",
    "            'num_items': len(movie_encoder.classes_),\n",
    "            'embedding_dim': model.hparams.embedding_dim,\n",
    "            'layers': model.hparams.layers,\n",
    "            'lr': model.hparams.lr\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save encoders separately to avoid potential pickle/torch.save conflicts\n",
    "    with open(save_dir / f\"checkpoint_epoch_{epoch}_encoders.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            'user_encoder': user_encoder,\n",
    "            'movie_encoder': movie_encoder\n",
    "        }, f)\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    checkpoint_path = save_dir / f\"checkpoint_epoch_{epoch}.pt\"\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model=None, optimizer=None):\n",
    "    \"\"\"\n",
    "    Load a training checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the checkpoint file\n",
    "        model (NCF, optional): The model to load the state into. If None, a new model will be created.\n",
    "        optimizer (torch.optim.Optimizer, optional): The optimizer to load the state into\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, epoch, metrics, user_encoder, movie_encoder)\n",
    "    \"\"\"\n",
    "    checkpoint_path = Path(checkpoint_path)\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    # Load or create model\n",
    "    if model is None:\n",
    "        model = NCF(**checkpoint['model_hparams'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Load optimizer state if provided\n",
    "    if optimizer is not None and checkpoint['optimizer_state_dict'] is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Load encoders\n",
    "    with open(checkpoint_path.parent / f\"{checkpoint_path.stem}_encoders.pkl\", \"rb\") as f:\n",
    "        encoders = pickle.load(f)\n",
    "        user_encoder = encoders['user_encoder']\n",
    "        movie_encoder = encoders['movie_encoder']\n",
    "    \n",
    "    print(f\"Checkpoint loaded: {checkpoint_path}\")\n",
    "    return model, checkpoint['epoch'], checkpoint['metrics'], user_encoder, movie_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fb0d7054-91d8-45e6-8f1c-5c19732767bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and encoders saved to saved_model\n",
      "Model and encoders loaded from saved_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_10064\\2240728140.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  config = torch.load(load_dir / \"model_config.pt\")\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_10064\\2240728140.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(load_dir / \"model.pt\")\n"
     ]
    }
   ],
   "source": [
    "save_model(model, user_encoder, movie_encoder, save_dir=\"saved_model\")\n",
    "\n",
    "# Later, to load the model\n",
    "model, user_encoder, movie_encoder = load_model(load_dir=\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99cf18f-61c4-40c1-8196-46b098470382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
